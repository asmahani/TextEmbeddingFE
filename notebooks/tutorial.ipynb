{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce5bc25-a5c5-4139-88ce-185e873b1646",
   "metadata": {},
   "source": [
    "# Using Text Fields in Predictive Models - An Application of Large Language Models via the *TextEmbeddingFE* Package\n",
    "\n",
    "In this tutorial, we illustrate how various functions in the *TextEmbeddingFE* Python package can be used for feature extraction from text fields, for interpreting the extracted information, and for including it in predictive models. For more details on the concepts, please see the paper [include link to arxiv or published paper]. Let's start with loading and exploring the data.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd19565-7009-4d06-bbc3-bfe1694ecf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnoses</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>is_female</th>\n",
       "      <th>weight</th>\n",
       "      <th>optime</th>\n",
       "      <th>aki_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155500. Cardiac conduit complication;010125. P...</td>\n",
       "      <td>18.11</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>80.9</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>091591. Aortic regurgitation;091519. Congenita...</td>\n",
       "      <td>18.23</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>56.1</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155516. Cardiac conduit failure;090101. Common...</td>\n",
       "      <td>16.86</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>61.6</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010116. Partial anomalous pulmonary venous con...</td>\n",
       "      <td>16.88</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>44.3</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155516. Cardiac conduit failure;010133. Left h...</td>\n",
       "      <td>18.12</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           diagnoses    age  height  \\\n",
       "0  155500. Cardiac conduit complication;010125. P...  18.11     148   \n",
       "1  091591. Aortic regurgitation;091519. Congenita...  18.23     169   \n",
       "2  155516. Cardiac conduit failure;090101. Common...  16.86     166   \n",
       "3  010116. Partial anomalous pulmonary venous con...  16.88     162   \n",
       "4  155516. Cardiac conduit failure;010133. Left h...  18.12     175   \n",
       "\n",
       "   is_female  weight  optime  aki_severity  \n",
       "0          1    80.9     112             0  \n",
       "1          1    56.1     144             1  \n",
       "2          1    61.6     114             0  \n",
       "3          1    44.3     109             0  \n",
       "4          0    70.5     119             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eac728-fd14-47f1-a69f-83caa9e1a251",
   "metadata": {},
   "source": [
    "The data corresponds to >800 pediatric patients undergoing cardiopulmonary bypass (CPB). Besides basic patient information (sex, age/height/weight at the time of operation), we have three additional field:\n",
    "- *diagnoses*: One or more standard diagnosis codes (including descriptions) for the patient, separated by ';'.\n",
    "- *optime*: Length of CPB operation in minutes.\n",
    "- *aki_severity*: A binary indicator of whether the patient suffered a severe form of post-operative acute kidney injury (AKI).\n",
    "The data included here is part of a larger collection; for a more detailed context on data collection see [this](https://www.medrxiv.org/content/10.1101/2024.03.18.24304520v1) paper.\n",
    "\n",
    "Our goal is to predict the postoperative outcome (*aki_severity*) from the remaining columns, which are all available before surgery. While age, height, gender, weight and operation time are all numeric, yet the *diagnoses* column is not, and hence cannot be directly used in a predictive model.\n",
    "\n",
    "## Options for Encoding Text\n",
    "\n",
    "- *Multivalued Binary Encoding*: Perhaps the most straightforward approach for encoding the *diagnoses* column is what can be described as *multivalued binary encoding* (MBE). We can assign a binary indicator to each individual diagnostic code, and to each patient we assign 1 to all diagnostic codes present, and 0 to the absent codes. This would be a generalzation of one-hot encoding, where multiple 1's can be present in the resulting binary vector for each observation, rather than only one.\n",
    "- *doc2vec*: [add description of gensim doc2vec, its genesis in word2vec, and its limitations]\n",
    "- *LLM embeddings*: Modern text-generating LLMs such as GPT-4 from OpenAI are deep neural networks, where the initial layers encode input text into a numeric representation. It is therefore possible to extract an embedding LLM from a text-generating LLM, followed by fine-tuning for embedding tasks. The resulting models tend to better utilize context for inferring the meaning of text and hence have improved semantic mapping in their embeddings.\n",
    "\n",
    "LLM embeddings have a couple of significant properties:\n",
    "1. They are *high-dimensional*. For instance, OpenAI's *text-embedding-3-large* model returns a vector of length 3072! Directly including this long vector in predictive problems with limited data can lead to problems such as long training time and overfitting.\n",
    "2. They are *normalized*, i.e., their norm is meaningless and instead only the direction that they point in the high-dimensional space is relevant.\n",
    "\n",
    "The tools offered in *TextEmbeddingFE* are designed to address the above two properties.\n",
    "\n",
    "## Token Counting\n",
    "\n",
    "Before proceeding, we mention a utility function, *count_tokens*, provided in *TextEmbeddingFE* for counting the tokens in a list of strings. It returns both the maximum number of tokens in the list of strings, and the total token count. They are useful fortwo purposes:\n",
    "1. Both embedding and text-generation models from OpenAI - and likely in other LLMs - have maximum limits on token count. For instance, as of this writing, OpenAI's embedding model *text-embedding-3-large* has a maximum input length of 8191 tokens (see [here](https://platform.openai.com/docs/guides/embeddings/embedding-models)). By comparing the maximum number of tokens in our list against these product specifications, we can ensure that no inadvertent truncation of the data and/or error would occur.\n",
    "2. We can also use the token count to estimate the costs, since prices are often expressed per token. For OpenAI's pricing details, see [here](https://openai.com/api/pricing).\n",
    "\n",
    "As an example, let's check the maximum and total token count for the *diagnoses* column in our data. Note that the name of OpenAI's embedding and text-generation must be specified - with default values used below - since each may use a different tokenizer library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e565d8e9-5955-4119-8fe6-74c383e09aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138, 24539)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TextEmbeddingFE.main import count_tokens\n",
    "count_tokens(\n",
    "    text_list = list(df['diagnoses'])\n",
    "    , openai_embedding_model = 'text-embedding-3-large'\n",
    "    , openai_textgen_model = 'gpt-4-turbo'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33664dc4-7fcf-49d7-8119-b861a86c7efd",
   "metadata": {},
   "source": [
    "We see that while the maximum token count in this text column is only 138, the total token count is 24,539 which is higher than the context length for many of OpenAI's models. For instance, the *gpt-4* model as of this writing has a context length of 8,192 (see [here](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)).\n",
    "\n",
    "## Using OpenAI to Embed Text\n",
    "\n",
    "*TextEmbeddingFE* offers a convenient function, *embed_text*, that calls OpenAI's API function for text embedding (accessed via the *openai* Python package), and returns a numpy matrix. To call *embed_text*, we first must have created an OpenAI API key, which we can use to create a connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb61b43-0922-4e18-bb79-526e85f4efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62979037-75f3-41d9-af5f-d0bbdec2ae4b",
   "metadata": {},
   "source": [
    "Note that the above assumes we have a valid OpenAI API key, saved under the environment-variable *OPENAI_API_KEY* to the file named *.env* in the current folder. For more on generating and using OpenAI API keys, see [this post](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key).\n",
    "\n",
    "We now embed the *diagnoses* column using the following call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a70ab3-a65d-4194-aed6-18da63793416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3072)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TextEmbeddingFE.main import embed_text\n",
    "\n",
    "my_embedding_matrix = embed_text(\n",
    "    openai_client = client\n",
    "    , text_list = list(df['diagnoses'][:5]) # submitting only the first 5 rows for illustration\n",
    "    , openai_embedding_model = 'text-embedding-3-large'\n",
    ")\n",
    "my_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a924e30-3951-44d5-b730-7d871fed78b3",
   "metadata": {},
   "source": [
    "Each row of the returned matrix is a 3,072-long embedding vector that provides a numeric representation of the diagnoses codes/descriptions for that patient. It is also easy to check that the embedding vectors are L2-normalized, and hence only represent a direction in the high-dimensional space, rather than magnitude information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c389a020-2fdd-4ebb-b6f4-4bf27584e061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000006, 1.00000001, 0.99999995, 0.99999983, 1.0000001 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.apply_along_axis(lambda x: sum(x * x), 1, my_embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a0348-7b89-4f3f-9bf1-f9f83a12d736",
   "metadata": {},
   "source": [
    "In the rest of this tutorial, we present two workflows for taking advantage of text embeddings returned by OpenAI's LLM. First is a explanatory workflow, focused on interpreting the embeddings and understanding what risk factors they impply. Secondly, we present a predictive workflow to incorporate the embeddings into a predictive model which would includes other variables. In both workflows, we take into consideration the normalized property of the embedding vectors.\n",
    "\n",
    "## Explanatory Workflow\n",
    "\n",
    "In this workflow, we use a text-generating LLM to intrepret the output of the embedding LLM. More specifically, we follow these steps:\n",
    "1. Cluster observations using the embedding vector as feature.\n",
    "2. Group the observations according the embedding-based clusters and assemble a prompt.\n",
    "3. Submit the prompt to OpenAI to solicit cluster labels/descriptions.\n",
    "\n",
    "### Clustering using Text Embeddings\n",
    "\n",
    "Ideally, we would like the clustering algorithm to use a magnitude-insensitive distance metric such as the cosine distance:\n",
    "$$\n",
    "d(\\mathbf{x}, \\mathbf{y}) = 1 - \\frac{\\mathbf{x}^T \\, \\mathbf{y}}{(\\mathbf{x}^T \\mathbf{x})^{1/2} \\, (\\mathbf{y}^T \\mathbf{y})^{1/2}}\n",
    "$$\n",
    "[continue to describe why normalization alone doesn't make regular kmeans equivalent to spherical kmeans, mention that in the future we may implement spherical kmeans, also that the sister R package currently has native spherical kmeans support]\n",
    "\n",
    "For illustration, we load a truncated embedding file for the *diagnoses* column, where the first 100 elements of the 3072-long embedding vector for each patient were extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0414cdb-0e45-4b4e-8295-a04231cb6a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_id</th>\n",
       "      <th>operation_no</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X91</th>\n",
       "      <th>X92</th>\n",
       "      <th>X93</th>\n",
       "      <th>X94</th>\n",
       "      <th>X95</th>\n",
       "      <th>X96</th>\n",
       "      <th>X97</th>\n",
       "      <th>X98</th>\n",
       "      <th>X99</th>\n",
       "      <th>X100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PR-00000001</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.004914</td>\n",
       "      <td>0.065536</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>-0.002728</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.034030</td>\n",
       "      <td>-0.065837</td>\n",
       "      <td>0.027918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022181</td>\n",
       "      <td>0.049798</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>-0.044452</td>\n",
       "      <td>0.028218</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>-0.058388</td>\n",
       "      <td>-0.022827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PR-00000002</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>0.067997</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.012815</td>\n",
       "      <td>0.031194</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022067</td>\n",
       "      <td>0.032728</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.025801</td>\n",
       "      <td>0.051881</td>\n",
       "      <td>-0.030496</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.010762</td>\n",
       "      <td>-0.061985</td>\n",
       "      <td>-0.023136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PR-00000003</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.024688</td>\n",
       "      <td>-0.002821</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.030009</td>\n",
       "      <td>0.014280</td>\n",
       "      <td>0.039548</td>\n",
       "      <td>-0.047434</td>\n",
       "      <td>0.019382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040968</td>\n",
       "      <td>0.022296</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>0.047869</td>\n",
       "      <td>0.081995</td>\n",
       "      <td>-0.024761</td>\n",
       "      <td>0.023021</td>\n",
       "      <td>0.020992</td>\n",
       "      <td>-0.043491</td>\n",
       "      <td>0.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PR-00000004</td>\n",
       "      <td>4</td>\n",
       "      <td>0.019465</td>\n",
       "      <td>0.059981</td>\n",
       "      <td>-0.004884</td>\n",
       "      <td>0.010679</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.057856</td>\n",
       "      <td>-0.021590</td>\n",
       "      <td>0.008793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022216</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.041841</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>0.063999</td>\n",
       "      <td>-0.021314</td>\n",
       "      <td>0.027341</td>\n",
       "      <td>-0.011377</td>\n",
       "      <td>-0.060039</td>\n",
       "      <td>-0.014886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR-00000005</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.052803</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>-0.010779</td>\n",
       "      <td>0.011241</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>-0.034958</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017798</td>\n",
       "      <td>0.046048</td>\n",
       "      <td>-0.010954</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>0.041459</td>\n",
       "      <td>-0.018786</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.013798</td>\n",
       "      <td>-0.066921</td>\n",
       "      <td>-0.013504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    project_id  operation_no        X1        X2        X3        X4  \\\n",
       "0  PR-00000001             1 -0.004914  0.065536  0.000462 -0.002728   \n",
       "1  PR-00000002             2 -0.003297  0.067997  0.001257  0.000272   \n",
       "2  PR-00000003             3 -0.024688 -0.002821  0.005994  0.030009   \n",
       "3  PR-00000004             4  0.019465  0.059981 -0.004884  0.010679   \n",
       "4  PR-00000005             5  0.006310  0.052803  0.001488 -0.010779   \n",
       "\n",
       "         X5        X6        X7        X8  ...       X91       X92       X93  \\\n",
       "0  0.007441  0.034030 -0.065837  0.027918  ... -0.022181  0.049798  0.003935   \n",
       "1  0.012815  0.031194 -0.037501  0.004168  ... -0.022067  0.032728 -0.007845   \n",
       "2  0.014280  0.039548 -0.047434  0.019382  ... -0.040968  0.022296  0.009749   \n",
       "3  0.000434  0.057856 -0.021590  0.008793  ... -0.022216  0.000365  0.041841   \n",
       "4  0.011241  0.012157 -0.034958  0.025509  ... -0.017798  0.046048 -0.010954   \n",
       "\n",
       "        X94       X95       X96       X97       X98       X99      X100  \n",
       "0  0.039526  0.032888 -0.044452  0.028218  0.003186 -0.058388 -0.022827  \n",
       "1  0.025801  0.051881 -0.030496  0.003235  0.010762 -0.061985 -0.023136  \n",
       "2  0.047869  0.081995 -0.024761  0.023021  0.020992 -0.043491  0.004360  \n",
       "3  0.014515  0.063999 -0.021314  0.027341 -0.011377 -0.060039 -0.014886  \n",
       "4  0.029525  0.041459 -0.018786  0.014388  0.013798 -0.066921 -0.013504  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfEmbeddings = pd.read_csv('../data/embeddings.csv')\n",
    "dfEmbeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdade18-07a7-4d2a-a7c7-3ce6fca7ffd6",
   "metadata": {},
   "source": [
    "We extract the embedding matrix from the above dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab19e24-b995-41fb-be26-6b1012c21697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextEmbeddingFE.main import cluster_embeddings\n",
    "\n",
    "my_embedding_colnames = [\"X\" + str(n+1) for n in range(100)]\n",
    "my_embedding_matrix = dfEmbeddings[my_embedding_colnames].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1801d8-f3cc-4202-a30a-dd1e22852448",
   "metadata": {},
   "source": [
    "We can now call the *cluster_embeddings* function. In addition to the embedding matrix, we should also specify two hyperparameters:\n",
    "- *n_clusters*: Number of clusters to use in the k-means algorithm.\n",
    "- *n_init*: Number of initializations of k-means to try. Th best solution - the one minimizing total within-cluster distances - is returned.\n",
    "\n",
    "[explain how these two hyperparameters can be selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "794b9663-3285-4bdf-9af0-1a38da4fc22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54, 141,  84,  85,  81,  46,  34, 220,  67, 151], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clusters = cluster_embeddings(\n",
    "    X = my_embedding_matrix\n",
    "    , n_clusters = 10\n",
    "    , n_init = 10\n",
    ")\n",
    "np.bincount(my_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7187f14-2e89-4ec3-a513-497c7856b651",
   "metadata": {},
   "source": [
    "### Assembling a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4d417-3d42-4d06-a234-e65b1836994e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
