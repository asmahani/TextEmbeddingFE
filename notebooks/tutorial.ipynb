{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce5bc25-a5c5-4139-88ce-185e873b1646",
   "metadata": {},
   "source": [
    "# Using Text Fields in Predictive Models - An Application of Large Language Models via the *TextEmbeddingFE* Package\n",
    "\n",
    "In this tutorial, we illustrate how various functions in the *TextEmbeddingFE* Python package can be used for feature extraction from text fields, for interpreting the extracted information, and for including it in predictive models. For more details on the concepts, please see the paper [include link to arxiv or published paper]. Let's start with loading and exploring the data.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd19565-7009-4d06-bbc3-bfe1694ecf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_female</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>optime</th>\n",
       "      <th>diagnoses</th>\n",
       "      <th>aki_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18.11</td>\n",
       "      <td>148</td>\n",
       "      <td>80.9</td>\n",
       "      <td>112</td>\n",
       "      <td>155500. Cardiac conduit complication;010125. P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.23</td>\n",
       "      <td>169</td>\n",
       "      <td>56.1</td>\n",
       "      <td>144</td>\n",
       "      <td>091591. Aortic regurgitation;091519. Congenita...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16.86</td>\n",
       "      <td>166</td>\n",
       "      <td>61.6</td>\n",
       "      <td>114</td>\n",
       "      <td>155516. Cardiac conduit failure;090101. Common...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>16.88</td>\n",
       "      <td>162</td>\n",
       "      <td>44.3</td>\n",
       "      <td>109</td>\n",
       "      <td>010116. Partial anomalous pulmonary venous con...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>18.12</td>\n",
       "      <td>175</td>\n",
       "      <td>70.5</td>\n",
       "      <td>119</td>\n",
       "      <td>155516. Cardiac conduit failure;010133. Left h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_female    age  height  weight  optime  \\\n",
       "0          1  18.11     148    80.9     112   \n",
       "1          1  18.23     169    56.1     144   \n",
       "2          1  16.86     166    61.6     114   \n",
       "3          1  16.88     162    44.3     109   \n",
       "4          0  18.12     175    70.5     119   \n",
       "\n",
       "                                           diagnoses  aki_severity  \n",
       "0  155500. Cardiac conduit complication;010125. P...             0  \n",
       "1  091591. Aortic regurgitation;091519. Congenita...             1  \n",
       "2  155516. Cardiac conduit failure;090101. Common...             0  \n",
       "3  010116. Partial anomalous pulmonary venous con...             0  \n",
       "4  155516. Cardiac conduit failure;010133. Left h...             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/raw.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eac728-fd14-47f1-a69f-83caa9e1a251",
   "metadata": {},
   "source": [
    "The data corresponds to >800 pediatric patients undergoing cardiopulmonary bypass (CPB). Besides basic patient information (sex, age/height/weight at the time of operation), we have three additional field:\n",
    "- *diagnoses*: One or more standard diagnosis codes (including descriptions) for the patient, separated by ';'.\n",
    "- *optime*: Length of CPB operation in minutes.\n",
    "- *aki_severity*: A binary indicator of whether the patient suffered a severe form of post-operative acute kidney injury (AKI).\n",
    "The data included here is part of a larger collection; for a more detailed context on data collection see [this](https://www.medrxiv.org/content/10.1101/2024.03.18.24304520v1) paper.\n",
    "\n",
    "Our goal is to predict the postoperative outcome (*aki_severity*) from the remaining columns, which are all available before surgery. While age, height, gender, weight and operation time are all numeric, yet the *diagnoses* column is not, and hence cannot be directly used in a predictive model.\n",
    "\n",
    "## Options for Encoding Text\n",
    "\n",
    "- *Multivalued Binary Encoding*: Perhaps the most straightforward approach for encoding the *diagnoses* column is what can be described as *multivalued binary encoding* (MBE). We can assign a binary indicator to each individual diagnostic code, and to each patient we assign 1 to all diagnostic codes present, and 0 to the absent codes. This would be a generalzation of one-hot encoding, where multiple 1's can be present in the resulting binary vector for each observation, rather than only one.\n",
    "- *doc2vec*: [add description of gensim doc2vec, its genesis in word2vec, and its limitations]\n",
    "- *LLM embeddings*: Modern text-generating LLMs such as GPT-4 from OpenAI are deep neural networks, where the initial layers encode input text into a numeric representation. It is therefore possible to extract an embedding LLM from a text-generating LLM, followed by fine-tuning for embedding tasks. The resulting models tend to better utilize context for inferring the meaning of text and hence have improved semantic mapping in their embeddings.\n",
    "\n",
    "LLM embeddings have a couple of significant properties:\n",
    "1. They are *high-dimensional*. For instance, OpenAI's *text-embedding-3-large* model returns a vector of length 3072! Directly including this long vector in predictive problems with limited data can lead to problems such as long training time and overfitting.\n",
    "2. They are *normalized*, i.e., their norm is meaningless and instead only the direction that they point in the high-dimensional space is relevant.\n",
    "\n",
    "The tools offered in *TextEmbeddingFE* are designed to address the above two properties.\n",
    "\n",
    "## Token Counting\n",
    "\n",
    "Before proceeding, we mention a utility function, *count_tokens*, provided in *TextEmbeddingFE* for counting the tokens in a list of strings. It returns both the maximum number of tokens in the list of strings, and the total token count. They are useful fortwo purposes:\n",
    "1. Both embedding and text-generation models from OpenAI - and likely in other LLMs - have maximum limits on token count. For instance, as of this writing, OpenAI's embedding model *text-embedding-3-large* has a maximum input length of 8191 tokens (see [here](https://platform.openai.com/docs/guides/embeddings/embedding-models)). By comparing the maximum number of tokens in our list against these product specifications, we can ensure that no inadvertent truncation of the data and/or error would occur.\n",
    "2. We can also use the token count to estimate the costs, since prices are often expressed per token. For OpenAI's pricing details, see [here](https://openai.com/api/pricing).\n",
    "\n",
    "As an example, let's check the maximum and total token count for the *diagnoses* column in our data. Note that the name of OpenAI's embedding and text-generation must be specified - with default values used below - since each may use a different tokenizer library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e565d8e9-5955-4119-8fe6-74c383e09aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138, 24539)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TextEmbeddingFE.main import count_tokens\n",
    "count_tokens(\n",
    "    text_list = list(df['diagnoses'])\n",
    "    , openai_embedding_model = 'text-embedding-3-large'\n",
    "    , openai_textgen_model = 'gpt-4-turbo'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33664dc4-7fcf-49d7-8119-b861a86c7efd",
   "metadata": {},
   "source": [
    "We see that while the maximum token count in this text column is only 138, the total token count is 24,539 which is higher than the context length for many of OpenAI's models. For instance, the *gpt-4* model as of this writing has a context length of 8,192 (see [here](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)).\n",
    "\n",
    "## Using OpenAI to Embed Text\n",
    "\n",
    "*TextEmbeddingFE* offers a convenient function, *embed_text*, that calls OpenAI's API function for text embedding (accessed via the *openai* Python package), and returns a numpy matrix. To call *embed_text*, we first must have created an OpenAI API key, which we can use to create a connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb61b43-0922-4e18-bb79-526e85f4efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62979037-75f3-41d9-af5f-d0bbdec2ae4b",
   "metadata": {},
   "source": [
    "Note that the above assumes we have a valid OpenAI API key, saved under the environment-variable *OPENAI_API_KEY* to the file named *.env* in the current folder. For more on generating and using OpenAI API keys, see [this post](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key).\n",
    "\n",
    "We now embed the *diagnoses* column using the following call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a70ab3-a65d-4194-aed6-18da63793416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3072)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TextEmbeddingFE.main import embed_text\n",
    "\n",
    "my_embedding_matrix = embed_text(\n",
    "    openai_client = client\n",
    "    , text_list = list(df['diagnoses'][:5]) # submitting only the first 5 rows for illustration\n",
    "    , openai_embedding_model = 'text-embedding-3-large'\n",
    ")\n",
    "my_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a924e30-3951-44d5-b730-7d871fed78b3",
   "metadata": {},
   "source": [
    "Each row of the returned matrix is a 3,072-long embedding vector that provides a numeric representation of the diagnoses codes/descriptions for that patient. It is also easy to check that the embedding vectors are L2-normalized, and hence only represent a direction in the high-dimensional space, rather than magnitude information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c389a020-2fdd-4ebb-b6f4-4bf27584e061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000006, 1.0000001 , 0.99999995, 0.99999983, 1.0000001 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.apply_along_axis(lambda x: sum(x * x), 1, my_embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a0348-7b89-4f3f-9bf1-f9f83a12d736",
   "metadata": {},
   "source": [
    "In the rest of this tutorial, we present two workflows for taking advantage of text embeddings returned by OpenAI's LLM. First is a explanatory workflow, focused on interpreting the embeddings and understanding what risk factors they impply. Secondly, we present a predictive workflow to incorporate the embeddings into a predictive model which would includes other variables. In both workflows, we take into consideration the normalized property of the embedding vectors.\n",
    "\n",
    "## Explanatory Workflow\n",
    "\n",
    "In this workflow, we use a text-generating LLM to intrepret the output of the embedding LLM. More specifically, we follow these steps:\n",
    "1. Cluster observations using the embedding vector as feature.\n",
    "2. Group the observations according the embedding-based clusters and assemble a prompt.\n",
    "3. Submit the prompt to OpenAI to solicit cluster labels/descriptions.\n",
    "\n",
    "### Clustering using Text Embeddings\n",
    "\n",
    "Ideally, we would like the clustering algorithm to use a magnitude-insensitive distance metric such as the cosine distance:\n",
    "$$\n",
    "d(\\mathbf{x}, \\mathbf{y}) = 1 - \\frac{\\mathbf{x}^T \\, \\mathbf{y}}{(\\mathbf{x}^T \\mathbf{x})^{1/2} \\, (\\mathbf{y}^T \\mathbf{y})^{1/2}}\n",
    "$$\n",
    "[continue to describe why normalization alone doesn't make regular kmeans equivalent to spherical kmeans, mention that in the future we may implement spherical kmeans, also that the sister R package currently has native spherical kmeans support]\n",
    "\n",
    "For illustration, we load a truncated embedding file for the *diagnoses* column, where the first 100 elements of the 3072-long embedding vector for each patient were extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0414cdb-0e45-4b4e-8295-a04231cb6a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X91</th>\n",
       "      <th>X92</th>\n",
       "      <th>X93</th>\n",
       "      <th>X94</th>\n",
       "      <th>X95</th>\n",
       "      <th>X96</th>\n",
       "      <th>X97</th>\n",
       "      <th>X98</th>\n",
       "      <th>X99</th>\n",
       "      <th>X100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004914</td>\n",
       "      <td>0.065536</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>-0.002728</td>\n",
       "      <td>0.007441</td>\n",
       "      <td>0.034030</td>\n",
       "      <td>-0.065837</td>\n",
       "      <td>0.027918</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>0.029449</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022181</td>\n",
       "      <td>0.049798</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>-0.044452</td>\n",
       "      <td>0.028218</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>-0.058388</td>\n",
       "      <td>-0.022827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019465</td>\n",
       "      <td>0.059981</td>\n",
       "      <td>-0.004884</td>\n",
       "      <td>0.010679</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.057856</td>\n",
       "      <td>-0.021590</td>\n",
       "      <td>0.008793</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>0.028753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022216</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.041841</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>0.063999</td>\n",
       "      <td>-0.021314</td>\n",
       "      <td>0.027341</td>\n",
       "      <td>-0.011377</td>\n",
       "      <td>-0.060039</td>\n",
       "      <td>-0.014886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.052803</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>-0.010779</td>\n",
       "      <td>0.011241</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>-0.034958</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>-0.012611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017798</td>\n",
       "      <td>0.046048</td>\n",
       "      <td>-0.010954</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>0.041459</td>\n",
       "      <td>-0.018786</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.013798</td>\n",
       "      <td>-0.066921</td>\n",
       "      <td>-0.013504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.024605</td>\n",
       "      <td>0.027766</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.025616</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.025818</td>\n",
       "      <td>-0.011927</td>\n",
       "      <td>0.065287</td>\n",
       "      <td>0.015571</td>\n",
       "      <td>0.043525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017621</td>\n",
       "      <td>0.022542</td>\n",
       "      <td>0.022470</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>-0.050567</td>\n",
       "      <td>0.058822</td>\n",
       "      <td>-0.009142</td>\n",
       "      <td>-0.051751</td>\n",
       "      <td>-0.005033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013725</td>\n",
       "      <td>0.064173</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>0.010255</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.012550</td>\n",
       "      <td>-0.011009</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>-0.015934</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008481</td>\n",
       "      <td>0.048487</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>0.027340</td>\n",
       "      <td>0.041392</td>\n",
       "      <td>-0.035852</td>\n",
       "      <td>0.039120</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>-0.061403</td>\n",
       "      <td>-0.009010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0 -0.004914  0.065536  0.000462 -0.002728  0.007441  0.034030 -0.065837   \n",
       "1  0.019465  0.059981 -0.004884  0.010679  0.000434  0.057856 -0.021590   \n",
       "2  0.006310  0.052803  0.001488 -0.010779  0.011241  0.012157 -0.034958   \n",
       "3 -0.024605  0.027766  0.000341  0.025616  0.003682  0.025818 -0.011927   \n",
       "4  0.013725  0.064173 -0.001580  0.010255  0.013958  0.012550 -0.011009   \n",
       "\n",
       "         X8        X9       X10  ...       X91       X92       X93       X94  \\\n",
       "0  0.027918  0.009296  0.029449  ... -0.022181  0.049798  0.003935  0.039526   \n",
       "1  0.008793  0.009179  0.028753  ... -0.022216  0.000365  0.041841  0.014515   \n",
       "2  0.025509 -0.003456 -0.012611  ... -0.017798  0.046048 -0.010954  0.029525   \n",
       "3  0.065287  0.015571  0.043525  ... -0.017621  0.022542  0.022470  0.042399   \n",
       "4  0.009173 -0.015934  0.016448  ... -0.008481  0.048487  0.014635  0.027340   \n",
       "\n",
       "        X95       X96       X97       X98       X99      X100  \n",
       "0  0.032888 -0.044452  0.028218  0.003186 -0.058388 -0.022827  \n",
       "1  0.063999 -0.021314  0.027341 -0.011377 -0.060039 -0.014886  \n",
       "2  0.041459 -0.018786  0.014388  0.013798 -0.066921 -0.013504  \n",
       "3  0.021228 -0.050567  0.058822 -0.009142 -0.051751 -0.005033  \n",
       "4  0.041392 -0.035852  0.039120  0.001916 -0.061403 -0.009010  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfEmbeddings = pd.read_csv('../data/embeddings.csv')\n",
    "dfEmbeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdade18-07a7-4d2a-a7c7-3ce6fca7ffd6",
   "metadata": {},
   "source": [
    "We extract the embedding matrix from the above dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab19e24-b995-41fb-be26-6b1012c21697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextEmbeddingFE.main import cluster_embeddings\n",
    "\n",
    "my_embedding_colnames = [\"X\" + str(n+1) for n in range(100)]\n",
    "my_embedding_matrix = dfEmbeddings[my_embedding_colnames].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1801d8-f3cc-4202-a30a-dd1e22852448",
   "metadata": {},
   "source": [
    "We can now call the `cluster_embeddings` function. In addition to the embedding matrix, we should also specify two hyperparameters:\n",
    "- `n_clusters`: Number of clusters to use in the k-means algorithm.\n",
    "- `n_init`: Number of initializations of k-means to try. Th best solution - the one minimizing total within-cluster distances - is returned.\n",
    "\n",
    "[explain how these two hyperparameters can be selected]\n",
    "\n",
    "[don't we need to normalize the embedding matrix first?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "794b9663-3285-4bdf-9af0-1a38da4fc22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 65, 194,  63,  19,  34,  61, 182, 128,  38,  46], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_clusters = cluster_embeddings(\n",
    "    X = my_embedding_matrix\n",
    "    , n_clusters = 10\n",
    "    , n_init = 10\n",
    ")\n",
    "np.bincount(my_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7187f14-2e89-4ec3-a513-497c7856b651",
   "metadata": {},
   "source": [
    "### Assembling a Prompt\n",
    "To assemble a prompt for soliciting cluster descriptions, we use the function `generate_prompt`. Required fields include:\n",
    "- `text_list`: The text column whose embeddings were used for clustering the observations.\n",
    "- `cluster_labels`: The vector of cluster labels - in consecutive integers starting at 0 - corresponding to the same observations in *text_list*.\n",
    "- `prompt_observations`: What the observations should be called, in plural form.\n",
    "- `prompt_texts`: What the text field values represent, also in plural form.\n",
    "\n",
    "The function returns the total token count for the prompt, followed by the prompt text. Below is an example call for our problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad4d417-3d42-4d06-a234-e65b1836994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token count: 25196\n"
     ]
    }
   ],
   "source": [
    "from TextEmbeddingFE.main import generate_prompt\n",
    "token_count, my_prompt = generate_prompt(\n",
    "    text_list = list(df['diagnoses'])\n",
    "    , cluster_labels = my_clusters\n",
    "    , prompt_observations = 'patients'\n",
    "    , prompt_texts = 'diagnoses'\n",
    ")\n",
    "print('token count:', token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee997df1-fd2d-47ab-b80c-0bcff5ea234c",
   "metadata": {},
   "source": [
    "Let's examine the prompt (we print the first 10 lines for brevity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fa44db9-cfbd-4c10-9e9e-1afe9313e1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a list of 830 patients. Text lines represent diagnoses. Patients have been grouped into 10 groups, according to their diagnoses. Please suggest group labels that are representative of their members, and also distinct from each other:\n",
      "\n",
      "=====\n",
      "\n",
      "Group 1:\n",
      "\n",
      "050402. ASD within oval fossa (secundum)\n",
      "050402. ASD within oval fossa (secundum);050402. ASD within oval fossa (secundum);110616. Congenital complete heart block;110506. Ventricular tachycardia\n",
      "151063. Residual ASD;050402. ASD within oval fossa (secundum);071001. Perimembranous central ventricular septal defect (VSD)\n",
      "050402. ASD within oval fossa (secundum);091613. Aortic root dilation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(my_prompt.splitlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3348129-aa9f-42d9-8a75-da28c8752b55",
   "metadata": {},
   "source": [
    "It contains a 'preamble' paragraph, followed by the list of patients, grouped into clusters, and for each patient the corresponding text field is printed. It is recommended that users explore the auto-generated preamble, and if they wish to edit it, override the default value for the `preamble` argument, perhaps via fine-tuning the auto-generated one. Note that, in this case, providing values for `prompt_observations` and `prompt_texts` is not necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f11515a1-c777-4891-9a01-f78fdf8ed90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_better_preamble = (\n",
    "    \"The following represents a group of pediatric patients undergoing cardiopulmonary bypass.\"\n",
    "    \" Each row contains one or more surgical procedures performed on the patient during bypass, separated by ';'.\"\n",
    "    \" Patients are grouped into 10 groups according to the similarity of their surgical procedures.\"\n",
    "    \" Please suggest group labels that are representative of their members, and also distinct from each other:\"\n",
    ")\n",
    "_, my_better_prompt = generate_prompt(\n",
    "    text_list = list(df['diagnoses'])\n",
    "    , cluster_labels = my_clusters\n",
    "    , preamble = my_better_preamble\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30a54471-b5cb-4f18-abbf-c185de8e875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following represents a group of pediatric patients undergoing cardiopulmonary bypass. Each row contains one or more surgical procedures performed on the patient during bypass, separated by ';'. Patients are grouped into 10 groups according to the similarity of their surgical procedures. Please suggest group labels that are representative of their members, and also distinct from each other:\n",
      "\n",
      "=====\n",
      "\n",
      "Group 1:\n",
      "\n",
      "050402. ASD within oval fossa (secundum)\n",
      "050402. ASD within oval fossa (secundum);050402. ASD within oval fossa (secundum);110616. Congenital complete heart block;110506. Ventricular tachycardia\n",
      "151063. Residual ASD;050402. ASD within oval fossa (secundum);071001. Perimembranous central ventricular septal defect (VSD)\n",
      "050402. ASD within oval fossa (secundum);091613. Aortic root dilation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(my_better_prompt.splitlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b27f3-4e8d-4bdb-be09-01f6dc08051d",
   "metadata": {},
   "source": [
    "### Summarizing the Clusters\n",
    "\n",
    "We are now ready to submit the above prompt to OpenAI and solicit cluster labels. Since our total token count of ~25,000 exceeds the context length for `gpt-4` (8192), we instead use `gpt-4-turbo` which has a much longer context of 128,000 tokens. As with text embedding API call, you can use the total token count to estimate the cost (which is about 25 cents for the above prompt as of this writing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e75faafc-c1b0-4071-8fc0-4ad76284dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextEmbeddingFE.main import interpret_clusters\n",
    "\n",
    "my_interpretation = interpret_clusters(\n",
    "    openai_client = client\n",
    "    , prompt = my_better_prompt\n",
    "    , openai_textgen_model = 'gpt-4-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baadf8b-db3b-4a06-a6f3-6e3bda21dd42",
   "metadata": {},
   "source": [
    "Let's examine the output of `gpt-4-turbo`, keeping in mind that the group numbers below are 1-indexed, while their corresponding cluster labels are 0-indexed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ecdb8c5-2068-4a1c-aab8-2ac43fe9419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the detailed surgical and procedural information for each group, the group labels can be proposed based on the most common or defining characteristics apparent from the procedures listed. Here are the suggested labels for each group, aiming for distinctness and representation:\n",
      "\n",
      "1. **Group 1: ASD (Atrial Septal Defect) Predominant Group**\n",
      "   - Most entries feature Atrial Septal Defect procedures, often combined with other heart structural issues.\n",
      "\n",
      "2. **Group 2: Aortic and Valvar Complications Group**\n",
      "   - Focuses on a variety of aortic issues, including regurgitations, stenosis, and other valvar complications.\n",
      "\n",
      "3. **Group 3: Tetralogy of Fallot Group**\n",
      "   - Centered around conditions and interventions related to Tetralogy of Fallot.\n",
      "\n",
      "4. **Group 4: Transposition of the Great Arteries (TGA) Group**\n",
      "   - Dedicated to cases involving the transposition of the great arteries with various associated complications.\n",
      "\n",
      "5. **Group 5: Double Outlet Right Ventricle (DORV) Group**\n",
      "   - Deals with surgical cases involving double outlet right ventricle configurations.\n",
      "\n",
      "6. **Group 6: Atrioventricular Septal Defect (AVSD) Group**\n",
      "   - Encompasses cases with atrioventricular septal defects, either complete or partial.\n",
      "\n",
      "7. **Group 7: Complex Cardiac Conduits and Regurgitations Group**\n",
      "   - Focuses on cardiac conduit failures, valve regurgitations, and pulmonary complications.\n",
      "\n",
      "8. **Group 8: VSD (Ventricular Septal Defect) Centric Group**\n",
      "   - Primarily involves cases with various forms of ventricular septal defects.\n",
      "\n",
      "9. **Group 9: Cardiomyopathy and Heart Muscle Disorders Group**\n",
      "   - Contains cases primarily dealing with various forms of cardiomyopathy.\n",
      "\n",
      "10. **Group 10: Multiple Systemic Disorders with Cardiac Involvement Group**\n",
      "    - Includes cases with widespread systemic issues affecting multiple cardiac structures and functions, often complex and multi-faceted.\n",
      "\n",
      "These labels intend to summarize the main focal points of the surgical interventions while distinguishing between the unique aspects of each group. Each label could potentially be further refined with more specific details from practitioners familiar with the cases.\n"
     ]
    }
   ],
   "source": [
    "print(my_interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35cef5-a326-464c-ad2f-60d40a4bd5c7",
   "metadata": {},
   "source": [
    "We can also use the `fisher_test_wrapper` function in the package to create a summary of the association of each cluster with the outcome variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45bbbace-f29e-4df1-b61a-1cf6535dc536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Odds Ratio</th>\n",
       "      <th>P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.894790</td>\n",
       "      <td>6.342603e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.138796</td>\n",
       "      <td>5.148787e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.870330</td>\n",
       "      <td>8.633978e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1.675128</td>\n",
       "      <td>7.328032e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>0.963667</td>\n",
       "      <td>9.134985e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.903133</td>\n",
       "      <td>8.801183e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>5.927835</td>\n",
       "      <td>2.053312e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0.586519</td>\n",
       "      <td>3.204134e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.122984</td>\n",
       "      <td>5.713400e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0.516493</td>\n",
       "      <td>4.300513e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category  Odds Ratio       P-value\n",
       "0         6    0.894790  6.342603e-01\n",
       "1         1    1.138796  5.148787e-01\n",
       "2         9    0.870330  8.633978e-01\n",
       "3         2    1.675128  7.328032e-02\n",
       "4         7    0.963667  9.134985e-01\n",
       "5         5    0.903133  8.801183e-01\n",
       "6         8    5.927835  2.053312e-07\n",
       "7         4    0.586519  3.204134e-01\n",
       "8         0    0.122984  5.713400e-06\n",
       "9         3    0.516493  4.300513e-01"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TextEmbeddingFE.main import fisher_test_wrapper\n",
    "fisher_test_wrapper(\n",
    "    dat = pd.DataFrame(\n",
    "        {'x': my_clusters, 'y': df['aki_severity']}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82ec8a-e7d2-4e6d-8933-26c723eafd89",
   "metadata": {},
   "source": [
    "By comparing the above table against the cluster labels produced by the LLM, we conclude, e.g., that:\n",
    "- Operations involving cardiomyopathies (Cluster 8 or Group 9) have the highest risk of severe AKI (OR = 5.9).\n",
    "- Operations for fixing aterial septal defects (ASD) (Cluster 0 or Group 1), on the other hand, have the lowest tisk of sever AKI (OR = 0.12).\n",
    "\n",
    "These observations can be subsequently validated in consultation with domain experts, and can also lead to new scientific hypotheses and follow-up research avenues.\n",
    "\n",
    "[mention that we can try to correct for other variables such as operation time in the above approach]\n",
    "\n",
    "## Predictive Workflow\n",
    "\n",
    "### Options for Using Text Embeddings\n",
    "\n",
    "Let's revisit the original, predictive problem: Our goal is to predictive a binary outcome - severe postoperative AKI. Besides the `diagnoses` text field, which we seek to transform to numeric via embedding, we also have a set of *baseline* variables: patient's gender/age/height/weight, and length of operation. As for including the embedding output, we can think of a few options:\n",
    "1. Including all 3072 elements of the embedding vectors in a model alongside the baseline variables is unlikely to be an optimal strategy, as the embedding features may dominate the model and lead to overfitting, given limited number of observations (830).\n",
    "2. We can instead apply clustering to the embedding vector, as done in the explanatory path, and include the resulting cluster labels as a categorical feature in the model (which are eventually transformed into a series of binary indicators via one-hot encoding).\n",
    "3. Alternatively, we can use the embedding vectors to predict the outcome variable, and include those predictions as a synthetic feature - and alongside the baseline variables - in our final model. This is akin to the idea of stacking in ML (add ref), though here we are not technically stacking a collection of model predictions, but rather a single model prediction is being stacked on top of a collection of baseline features.\n",
    "\n",
    "The last approach is what we docus on next. For more on comparing the above-mentioned approaches, see our paper (add link later). Importantly, in stacking we must use cross-validation to generate out-of-sample predictions within the training data; otherwise, we could inject a good amount of overfitting into the final model. The CV machinery and other nuances are handled in the `FeatureExtractor_BinaryClassifier` class in our package.\n",
    "\n",
    "The class constructor is a thin wrapper around the `KNeighborsClassifier` class constructor from the `scikit-learn` package. While we can pass any valid arguments to the underlying class, below we only specify the number of neighbors: (we need to explain why KNN is a good choice, i.e., a flexible alternative to the rigid clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f6e1fc8-5d26-4dd3-9ad7-c1c4d1907998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextEmbeddingFE.main import FeatureExtractor_BinaryClassifier\n",
    "my_fe = FeatureExtractor_BinaryClassifier(n_neighbors = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac00e5-4f26-4709-8883-84beb719963d",
   "metadata": {},
   "source": [
    "We will now illustrate that, adding a feature extracted from text embeddings via the `FeatureExtractor_BinaryClassifier` class to the baseline variables improves the discriminative performance of the model predicting severe AKI. The process illustrates our recommended approach for incorporating text fields in predictive models.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Before proceeding to creating a synthetic risk score from embeddings and doing performance comparison, we do two things:\n",
    "1. Extract matrix of baseline variables, matrix of text embeddings, and vector of response variable;\n",
    "2. Split all three of the above arrays into train and test sets by allocating a random 30% of observations to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c98b03ff-902c-4876-a4cf-1e2cbb4a7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedding = dfEmbeddings[my_embedding_colnames].to_numpy()\n",
    "X_baseline = df.loc[:, ['is_female', 'age', 'height', 'weight', 'optime']].to_numpy()\n",
    "y = df['aki_severity'].to_numpy(dtype = 'int')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_embedding_train, X_embedding_test, X_baseline_train, X_baseline_test, y_train, y_test = train_test_split(X_embedding, X_baseline, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2eb50d-c11d-4718-b373-760767242759",
   "metadata": {},
   "source": [
    "### Creating Embedding-Based Risk Score\n",
    "\n",
    "We now train our `FeatureExtractor_Classifier` model on the training set. The parameter `cv` specifies the number of CV folds used inside the `fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "634035f0-bc9d-4513-8c72-533a4a03c7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextEmbeddingFE.main.FeatureExtractor_Classifier at 0x1d25474e710>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_fe.fit(X = X_embedding_train, y = y_train, cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a09b5-43f3-486e-a745-69d1aef38ee9",
   "metadata": {},
   "source": [
    "To obtain the risk score for the training set, we call the `predict_proba` method, without passing any argument to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e67a28a-a0dd-4c9a-8110-e9ca30373d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = my_fe.predict_proba()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a700099-a426-4b99-82d1-64229036d8e1",
   "metadata": {},
   "source": [
    "To get the risk score for the test set, we pass the feature (i.e., embedding) matrix for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5ad4637-d773-4183-9df2-c110456641d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = my_fe.predict_proba(X = X_embedding_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd18c3e-0cab-45bc-9837-9d5dc550bcf8",
   "metadata": {},
   "source": [
    "It is important to note that it is incorrect to pass `X_embedding_train` to `predict_proba` to obtain `z_train` as the results are not the same. This can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8be85058-751e-472a-94ef-311d2615d1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.28, 0.26, 0.2 , 0.28, 0.2 ]),\n",
       " array([0.254, 0.238, 0.226, 0.26 , 0.182]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    my_fe.predict_proba()[:5]\n",
    "    , my_fe.predict_proba(X = X_embedding_train)[:5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65db900-40fd-4d9f-a3f4-aa8a016528e8",
   "metadata": {},
   "source": [
    "(provide further explanation of the mechanics of in-sample vs. out-of-sample prediction)\n",
    "\n",
    "### Training the Final Model\n",
    "\n",
    "We can now add the synthesized risk score to our baseline features to train the final model. In this tutorial, we use logistic regression for illustration, but more complex ML models can be used in real-world applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06182d75-cf00-410c-9bcf-69b49a9d5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_train = np.column_stack((X_baseline_train, z_train))\n",
    "X_combined_test = np.column_stack((X_baseline_test, z_test))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "my_model_combined = LogisticRegression(penalty = None, solver = 'newton-cholesky').fit(X_combined_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5e227-e1a5-4cbf-9880-a4646e9a8cd9",
   "metadata": {},
   "source": [
    "Note that we overrode two default setting for the logistic regression model. First, we switched off the default L2 regularization. Secondly, we changed the solver from L-BFGS to Newton optimization using Choleksy decomposition. A full discussion of these topics is beyond the scope of this tutorial, but briefly speaking, the decisions are driven by having relatively few features in the model, obviating any concern about overfitting and lack of stability in second-order methods such as Newton.\n",
    "\n",
    "Next, we train a reference model without the embedding-based risk score, and compare its discriminative performance against the above model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8031fb4f-d489-48f2-97bf-d7ed2f59795a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6704391891891891, 0.6805743243243243)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_reference = LogisticRegression(penalty = None, solver = 'newton-cholesky').fit(X_baseline_train, y_train)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "my_roc_reference = roc_auc_score(y_test, my_model_reference.predict_proba(X_baseline_test)[:, 1])\n",
    "my_roc_combined = roc_auc_score(y_test, my_model_combined.predict_proba(X_combined_test)[:, 1])\n",
    "\n",
    "(my_roc_reference, my_roc_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b558c5-4d18-4741-ad48-2e3180babedd",
   "metadata": {},
   "source": [
    "We see that adding the synthetic risk score improved the out-of-sample area under ROC by ~1%. This can be more systematically tested, e.g. by using a repeated cross-validation scheme.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we provided a step-by-step demosntration of how the `TextEmbeddingFE` Python package can be used for extracting information from text fields towards explanatory and predictive purposes. For more details on methodological aspects, see the paper (add link to arxiv paper when available)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
